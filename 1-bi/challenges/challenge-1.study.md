# vectorizer = CountVectorizer()

- NÃ£o Ã© um algorÃ­timo, Ã© sÃ³ um prÃ©-processador de texto â†’ transforma em nÃºmeros
- Ideal para modelos simples ou para quem estÃ¡ comeÃ§ando
- Interpreta sÃ³ a frequÃªncia de palavras, nÃ£o contexto
- Mas ele sozinho nÃ£o classifica â€” ele transforma o texto em nÃºmeros para que um modelo (como Naive Bayes, SVM, etc.) possa aprender padrÃµes.

### Geralmente vocÃª:

> - Coleta exemplos reais de spam e nÃ£o-spam 
> - Usa CountVectorizer para transformar os textos 
> - Treina um modelo de classificaÃ§Ã£o (ex: MultinomialNB, LogisticRegression)
> - Esse modelo aprende quais padrÃµes de palavras sÃ£o comuns em spam 
> - Depois, vocÃª usa o modelo para prever se um novo texto Ã© spam
 
### Exemplo
```
from sklearn.feature_extraction.text import CountVectorizer

textos = ["gosto de sorvete", "sorvete Ã© bom"]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(textos)

vectorizer.get_feature_names_out()
# ['bom', 'de', 'gosto', 'sorvete', 'Ã©']
```

### Matriz gerada
|                                 | bom | de | gosto | sorvete | Ã© |
| ------------------------------- | --- | -- | ----- | ------- | - |
| **Texto 1**: "gosto de sorvete" | 0   | 1  | 1     | 1       | 0 |
| **Texto 2**: "sorvete Ã© bom"    | 1   | 0  | 0     | 1       | 1 |

### âš ï¸ LimitaÃ§Ãµes
Ele nÃ£o considera contexto ou ordem das palavras

Trata palavras diferentes com o mesmo peso, mesmo que algumas sejam mais importantes

Frases com mesmo vocabulÃ¡rio em ordens diferentes terÃ£o o mesmo vetor

Para resolver essas limitaÃ§Ãµes, usa-se tÃ©cnicas mais avanÃ§adas como TfidfVectorizer, word2vec, BERT, etc.

Se quiser, posso mostrar a diferenÃ§a entre o CountVectorizer e o TfidfVectorizer.

# MultinomialNB - Naive Bayes

Ã‰ um algoritimo de aprendizagem supervisionada que aprende conforme a classificaÃ§Ã£o
Ã‰ uma variante especÃ­fica do algoritmo Naive Bayes, feita para lidar com dados discretos â€” como contagens de palavras, que Ã© exatamente o tipo de dado que o CountVectorizer gera.

ğŸ“š Tipos de Naive Bayes (em sklearn):

| Classe          | Quando usar                                      | Exemplo tÃ­pico                  |
| --------------- | ------------------------------------------------ | ------------------------------- |
| `MultinomialNB` | Para dados de **contagem** (como palavras)       | ClassificaÃ§Ã£o de texto, spam    |
| `GaussianNB`    | Para dados **contÃ­nuos** com distribuiÃ§Ã£o normal | Dados numÃ©ricos (ex: sensores)  |
| `BernoulliNB`   | Para **valores binÃ¡rios** (0 ou 1)               | PresenÃ§a ou ausÃªncia de palavra |



> Naive Bayes Ã© um mÃ©todo de machine learning que usa a frequÃªncia das ocorrÃªncias nos dados para prever uma variÃ¡vel de interesse. Ele se baseia no pensamento bayesiano, que propÃµe que nossas crenÃ§as devem ser ajustadas conforme novas evidÃªncias surgem. Por exemplo, se vocÃª acredita que todos os gansos sÃ£o brancos porque sempre viu gansos brancos ou porque alguÃ©m lhe disse isso, essa crenÃ§a muda ao encontrar um ganso preto. Com mais gansos pretos, sua crenÃ§a se atualiza novamente. Isso ilustra como o conhecimento evolui com a experiÃªncia. O modelo Ã© chamado de "naive" (ingÃªnuo) porque parte da suposiÃ§Ã£o de que as variÃ¡veis sÃ£o independentes entre si e nÃ£o considera conhecimento prÃ©vio; ele constrÃ³i as probabilidades a partir dos prÃ³prios dados disponÃ­veis.

